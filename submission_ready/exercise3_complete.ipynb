{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise3_complete",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Exercise 3 in Data-Oriented Programming Paradigms - Group 42\n",
        "# On the Evolution of Nuclear Energy Use\n",
        "\n",
        "\n",
        "Structure:\n",
        "\n",
        "* Overview of version + modules required to run\n",
        "* Which questions are we trying to answer\n",
        "* Datasets, what is in them, why we chose them\n",
        "* Data processing & exploration\n",
        "* q1\n",
        "* q2\n",
        "* q3\n",
        "* q4\n",
        "* conclusions\n",
        "* discussion on problems with data & biases, tools & techniques learned, work division\n",
        "\n",
        "Everything should run in this notebook, using the folder structure within this directory\n"
      ],
      "metadata": {
        "id": "aZQlmcfkWDGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing neceessary modules\n",
        "!pip install pyreadstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "Ivi2L5MhuBk5",
        "outputId": "468bc0d2-a874-4fa7-ce1b-2750e0e38305"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyreadstat\n",
            "  Downloading pyreadstat-1.1.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting pandas>=1.2.0\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 23.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadstat) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadstat) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyreadstat) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyreadstat) (1.15.0)\n",
            "Installing collected packages: pandas, pyreadstat\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.3.5 pyreadstat-1.1.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyreadstat\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "s0h63rGRYiz8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integrated Values Survey\n",
        "(by: Dario Giovannini)\n",
        "\n",
        "The World Values Survey and European Values Survey are two long-running projects, which each collect similar survey data from across the entire world, and more specifically europe respectively. This data is made available in various formats, but separately as it is aggregated by different institutions. The combination of these two datasets is known the Integrated Values Survey.\n",
        "\n",
        "In order to enable users to perform this combination in a consistent manner, a Merge Syntax is provided. Original data, the merge syntax files, and instructions on how to use them can be found here: https://www.worldvaluessurvey.org/WVSEVStrend.jsp\n",
        "\n",
        "Actually applying this merge syntax turned out to be quite tricky, as I was not familiar with the STATA and SPSS datatypes, but the merge syntax only exists for these. A consirable amount of trial-and-error as well as googling lead me to a proprietary software by IBM that deals with SPSS data, found here: https://www.ibm.com/products/spss-statistics\n",
        "\n",
        "Luckily, this software exists as a trial version, which was used to successfully apply the merge syntax, creating the integrated values survey datafile. This can be downloaded at the place indicated in the install_data.txt file. \n",
        "\n",
        "This data was then explored somwhat, to get a first feel for the overall structure of data and especially missing data:"
      ],
      "metadata": {
        "id": "V4agah-JreGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IVS is loaded\n",
        "\n",
        "ivs_data, ivs_meta = pyreadstat.read_sav(\"Integrated_values_surveys_1981-2021.sav\", encoding=\"cp850\")"
      ],
      "metadata": {
        "id": "F79h7fgktveE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at rough distribution of data in time intervals\n",
        "\n",
        "(ivs_data[\"S020\"].astype(int)//5 * 5).value_counts().sort_index(ascending=False).plot.barh()\n",
        "plt.title(\"Number of Survey Responses per 5-Year Interval\")"
      ],
      "metadata": {
        "id": "ncEcSItEuXH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# country names are converted to ISO-standard 3-letter-codes, which didn't exist in the original data in a unified way\n",
        "\n",
        "iso_codes = pd.read_csv(\"iso country codes/iso3166.tsv\", sep=\"\\t\")\n",
        "iso_codes[\"Numeric\"] = iso_codes[\"Numeric\"].fillna(0).astype(int)\n",
        "alpha2_to_alpha3 = iso_codes.set_index(\"Alpha-2 code\")[\"Alpha-3 code\"].to_dict()\n",
        "\n",
        "def map_codes(alpha2val):\n",
        "    if alpha2val in alpha2_to_alpha3:\n",
        "        return alpha2_to_alpha3[alpha2val]\n",
        "    else:\n",
        "        return \"invalid\"\n",
        "\n",
        "ivs_data[\"country\"] = ivs_data[\"S009\"].apply(map_codes)\n",
        "\n",
        "# since the 1985 inteval has so few responses, it is combined with the 1980 one.\n",
        "df[\"year\"] = (df[\"S020\"].astype(int)//5 * 5)\n",
        "df[\"year\"][df[\"year\"] == 1985] = 1980"
      ],
      "metadata": {
        "id": "Vhm2MwXluiVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "responses_per_country_per_interval = df[\"country\"].groupby(df[\"year\"]).apply(lambda x: x.value_counts().sort_values()).unstack(level=0).fillna(0).astype(int)\n",
        "\n",
        "share_of_invalid_responses = responses_per_country_per_interval.loc[\"invalid\"] / responses_per_country_per_interval.iloc[:-1].sum()\n",
        "print(share_of_invalid_responses)\n",
        "\n",
        "responses_per_country_per_interval.loc[[\"AUT\", \"DEU\", \"SWE\", \"FRA\", \"RUS\", \"TUR\", \"CHN\", \"IND\", \"JPN\", \"IRN\", \"USA\", \"BRA\", \"CAN\", \"MEX\"]]"
      ],
      "metadata": {
        "id": "P3jYNibZu-39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "the share of responses from unrecognized (as per ISO-3166) countries per time-interval is fairly small, and as might be expected more often found in the older parts of the dataset. Looking at a small sample of potentially interesting countries, none are present in all time-intervals, which indicates potential issues with continuity in the data.\n",
        "\n",
        "Next, a look is taken at the share of missing data for various questions deemed interesting (Please refer to the EVS_WVS_Dictionary_IVS file for details on these questions beyond the short comment given here):"
      ],
      "metadata": {
        "id": "9tWevvRvvcu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interesting_questions = [\"A001\", # family\n",
        "                         \"A002\", # friends\n",
        "                         \"A003\", # leisure time\n",
        "                         \"A004\", # politics\n",
        "                         \"A005\", # work\n",
        "                         \"A006\", # religion\n",
        "                         \"A010\", # happiness\n",
        "                         \"A165\", # most people can be trusted\n",
        "                         \"B008\", # protecting environment vs econ growth\n",
        "                         \"D059\", \"D060\", # sexism\n",
        "                         \"E069_04\", # confidence in press\n",
        "                         \"E069_11\", # confidence in government\n",
        "                         \"E069_14\", # confidence in environmental protection movement\n",
        "                         \"E235\", # importance of democracy\n",
        "                         \"F034\", # religious person (maybe redundant with A006)\n",
        "                         \"G006\", # proud of nationality\n",
        "                         ]\n",
        "# share of non-responses - all these questions have responses on a scale from 1-x, where 0 or negative values are considered non-responses of various descriptions.\n",
        "by_interval = df[interesting_questions].applymap(lambda x: x if x > 0 else np.nan).isna().groupby(df[\"year\"])\n",
        "non_responses = (by_interval.sum() / by_interval.count().max()).T\n",
        "non_responses"
      ],
      "metadata": {
        "id": "z3f-1QgHv0UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of these questions have fairly high rates of non-responses in the 1980 interval notable exceptions are A165 (general trust) and F034 (religious person). Other questions undergo large fluctuations. Overall, A001-A006, A165, E069_04, F034 and G006 seem like the most reliable values in terms of the share of missing values."
      ],
      "metadata": {
        "id": "RBVLo71YwINR"
      }
    }
  ]
}